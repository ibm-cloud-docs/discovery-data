---

copyright:
  years: 2020
lastupdated: "2020-12-04"

subcollection: discovery-data

---

{:shortdesc: .shortdesc}
{:external: target="_blank" .external}
{:tip: .tip}
{:note: .note}
{:pre: .pre}
{:important: .important}
{:deprecated: .deprecated}
{:codeblock: .codeblock}
{:screen: .screen}
{:download: .download}
{:hide-dashboard: .hide-dashboard}
{:apikey: data-credential-placeholder='apikey'} 
{:url: data-credential-placeholder='url'}
{:curl: .ph data-hd-programlang='curl'}
{:javascript: .ph data-hd-programlang='javascript'}
{:java: .ph data-hd-programlang='java'}
{:python: .ph data-hd-programlang='python'}
{:ruby: .ph data-hd-programlang='ruby'}
{:swift: .ph data-hd-programlang='swift'}
{:go: .ph data-hd-programlang='go'}

# Using a Cloud Pak for Data custom crawler plug-in with the Discovery tooling
{: #crawler-plugin-tooling}

![Cloud Pak for Data only](images/cpdonly.png) After you build and deploy a crawler plug-in, you can configure your {{site.data.keyword.discoveryshort}} collection to use your plug-in to process documents.
{: shortdesc}

You can create and manage a collection as described in [Creating and managing collections](/docs/discovery-data?topic=discovery-data-collections). You can select a successfully deployed crawler plug-in when you create and manage a collection. For more information, see [Crawler plug-in settings](/docs/discovery-data?topic=discovery-data-collection-types#plugin-settings). You can also deploy a crawler plug-in package to a testing environment.
